{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets, transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
        "!pip install wandb\n",
        "import wandb\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "h_CSGDz9h9Fu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128,128)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "train_dataset=datasets.ImageFolder(root='inaturalist_12K/train',transform=transform)\n",
        "test_dataset=datasets.ImageFolder(root='inaturalist_12K/val',transform=transform)\n",
        "\n",
        "train_indices, val_indices = train_test_split(list(range(len(train_dataset))), test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "val_sampler = SubsetRandomSampler(val_indices)\n",
        "# test_sampler= SequentialSampler(test_dataset)\n",
        "\n",
        "# Creating DataLoader instances for training and validation\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, sampler=train_sampler)\n",
        "val_loader = DataLoader(train_dataset, batch_size=32, sampler=val_sampler)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)"
      ],
      "metadata": {
        "id": "GgK450sQiDGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKs0jhB4foKz"
      },
      "outputs": [],
      "source": [
        "def ResNet50_uptil_k(k,classes):\n",
        "    model = models.resnet50(pretrained=True)\n",
        "\n",
        "    model_params=list(model.parameters())\n",
        "    for parameter in model_params[:k]:\n",
        "        parameter.requires_grad=False\n",
        "\n",
        "\n",
        "    num_filters=model.fc.in_features\n",
        "    model.fc=torch.nn.Linear(num_filters,classes)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def RESNET50(NUM_OF_CLASSES):\n",
        "    model = models.resnet50(pretrained=True)\n",
        "    num_ftrs = model.fc.in_features\n",
        "    model.fc = torch.nn.Linear(num_ftrs, NUM_OF_CLASSES)\n",
        "\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    for param in model.fc.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "5VVH1dWohbxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SmallCNN(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_filters,\n",
        "                 activation,\n",
        "                 data_augmentation,\n",
        "                 batch_normalization,\n",
        "                 dense_neurons,\n",
        "                 dropout\n",
        "                ):\n",
        "        self.activation=activation;\n",
        "        super(SmallCNN, self).__init__()\n",
        "\n",
        "        # 1st convolutional layer\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=num_filters, kernel_size=3, padding=1)\n",
        "        # 2nd convolutional layer\n",
        "        self.conv2 = nn.Conv2d(num_filters, num_filters * 2, kernel_size=3, padding=1)\n",
        "        # 3rd convolutional layer\n",
        "        self.conv3 = nn.Conv2d(num_filters * 2, num_filters * 4, kernel_size=3, padding=1)\n",
        "        # 4th convolutional layer\n",
        "        self.conv4 = nn.Conv2d(num_filters * 4, num_filters * 8, kernel_size=3, padding=1)\n",
        "        # 5th convolutional layer\n",
        "        self.conv5 = nn.Conv2d(num_filters * 8, num_filters * 16, kernel_size=3, padding=1)\n",
        "        # max pooling\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        # dense layer\n",
        "        self.fc1 = nn.Linear(num_filters * 16 * 4*4, dense_neurons)\n",
        "        # output dense layer\n",
        "        self.fc2 = nn.Linear(dense_neurons, 10)  # Output layer with 10 neurons for classification\n",
        "\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "        self.batch_norm=nn.BatchNorm2d(num_filters) if batch_normalization else None\n",
        "\n",
        "    def forward(self, x):\n",
        "        if(self.activation=='ReLU'):\n",
        "            x = self.pool(F.relu(self.conv1(x)))\n",
        "            x = self.pool(F.relu(self.conv2(x)))\n",
        "            x = self.pool(F.relu(self.conv3(x)))\n",
        "            x = self.pool(F.relu(self.conv4(x)))\n",
        "            x = self.pool(F.relu(self.conv5(x)))\n",
        "        elif(self.activation=='sigmoid'):\n",
        "            x = self.pool(F.sigmoid(self.conv1(x)))\n",
        "            x = self.pool(F.sigmoid(self.conv2(x)))\n",
        "            x = self.pool(F.sigmoid(self.conv3(x)))\n",
        "            x = self.pool(F.sigmoid(self.conv4(x)))\n",
        "            x = self.pool(F.sigmoid(self.conv5(x)))\n",
        "        elif(self.activation=='tanh'):\n",
        "            x = self.pool(F.tanh(self.conv1(x)))\n",
        "            x = self.pool(F.tanh(self.conv2(x)))\n",
        "            x = self.pool(F.tanh(self.conv3(x)))\n",
        "            x = self.pool(F.tanh(self.conv4(x)))\n",
        "            x = self.pool(F.tanh(self.conv5(x)))\n",
        "\n",
        "        elif (self.activation == 'GELU'):\n",
        "            x = self.pool(F.gelu(self.conv1(x)))\n",
        "            x = self.pool(F.gelu(self.conv2(x)))\n",
        "            x = self.pool(F.gelu(self.conv3(x)))\n",
        "            x = self.pool(F.gelu(self.conv4(x)))\n",
        "            x = self.pool(F.gelu(self.conv5(x)))\n",
        "        elif (self.activation == 'SiLU'):\n",
        "            x = self.pool(F.silu(self.conv1(x)))\n",
        "            x = self.pool(F.silu(self.conv2(x)))\n",
        "            x = self.pool(F.silu(self.conv3(x)))\n",
        "            x = self.pool(F.silu(self.conv4(x)))\n",
        "            x = self.pool(F.silu(self.conv5(x)))\n",
        "        elif (self.activation == 'Mish'):\n",
        "            x = self.pool(F.mish(self.conv1(x)))\n",
        "            x = self.pool(F.mish(self.conv2(x)))\n",
        "            x = self.pool(F.mish(self.conv3(x)))\n",
        "            x = self.pool(F.mish(self.conv4(x)))\n",
        "            x = self.pool(F.mish(self.conv5(x)))\n",
        "\n",
        "        x = x.view(-1, self.num_filters * 16 * 4*4)\n",
        "\n",
        "        if(self.activation=='ReLU'):\n",
        "            x = self.dropout(F.relu(self.fc1(x)))\n",
        "        elif(self.activation=='sigmoid'):\n",
        "            x = self.dropout(F.sigmoid(self.fc1(x)))\n",
        "        elif(self.activation=='tanh'):\n",
        "            x = self.dropout(F.tanh(self.fc1(x)))\n",
        "\n",
        "        elif(self.activation=='GELU'):\n",
        "            x = self.dropout(F.gelu(self.fc1(x)))\n",
        "        elif(self.activation=='SiLU'):\n",
        "            x = self.dropout(F.silu(self.fc1(x)))\n",
        "        elif(self.activation=='Mish'):\n",
        "            x = self.dropout(F.mish(self.fc1(x)))\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    def num_flat_features(self, x):\n",
        "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
        "        num_features = 1\n",
        "        for s in size:\n",
        "            num_features *= s\n",
        "        return num_features\n"
      ],
      "metadata": {
        "id": "51F5JvQSiUvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pretrain_model():\n",
        "  with wandb.init(project='cs23m035_DL_Assignment2') as run:\n",
        "    config=wandb.config\n",
        "\n",
        "    if(config.freeze=='False'):\n",
        "      cnn_model=RESNET50(10).to(device=device)\n",
        "    else:\n",
        "      cnn_model=RESNET50(config.freeze_value,10).to(device=device)\n",
        "\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer=optim.Adam(cnn_model.parameters(),lr=0.0001)\n",
        "\n",
        "\n",
        "    for i in range(10):\n",
        "\n",
        "        train_loss=0.0\n",
        "        train_correct=0\n",
        "        train_total=0\n",
        "        for image,label in train_loader:\n",
        "            image=image.to(device=device)\n",
        "            label=label.to(device=device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            scores=cnn_model(image)\n",
        "            loss=criterion(scores,label)\n",
        "\n",
        "            loss.backward()\n",
        "            #gradient descent or adam step\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss+=loss.item()\n",
        "            _,predicted=scores.max(1)\n",
        "            train_total+=label.size(0)\n",
        "            train_correct+=predicted.eq(label).sum().item()\n",
        "        train_loss=train_loss/len(train_loader)\n",
        "        train_accuracy=100*train_correct/train_total\n",
        "\n",
        "        num_correct=0\n",
        "        num_loss=0\n",
        "        total=0\n",
        "        cnn_model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for x,y in val_loader:\n",
        "                x=x.to(device=device)\n",
        "                y=y.to(device=device)\n",
        "                # x=x.reshape(x.shape[0],-1)\n",
        "                scores=cnn_model(x)\n",
        "                loss=criterion(scores,y)\n",
        "\n",
        "                num_loss+=loss.item()\n",
        "                _,predictions=scores.max(1)\n",
        "                total+=y.size(0)\n",
        "                num_correct+=predictions.eq(y).sum().item()\n",
        "        val_accuracy=100*num_correct/total\n",
        "        val_loss=num_loss/len(val_loader)\n",
        "        wandb.log({\"Train_Accuracy\" : train_accuracy,\"Train_Loss\" : train_loss,\"Validation_acc\" : val_accuracy,\"validation_loss\" : val_loss,'epoch':i})\n",
        "        print(f\"Train_Accuracy : {train_accuracy},Train_Loss : {train_loss}, Validation_acc : {val_accuracy},validation_loss : {val_loss},epoch:{i}\")\n",
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "GPb7YswThh3X",
        "outputId": "6dcd6343-b6fa-48e4-f833-4afafd1f4c31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'torch' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-592165fb4c5a>\u001b[0m in \u001b[0;36m<cell line: 61>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;31m# Create the CNN model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config={\n",
        "    'method':'bayes',\n",
        "    'name':'Parthu',\n",
        "    'metric':{\n",
        "        'goal':'maximize',\n",
        "        'name':'Validation_acc',\n",
        "    },\n",
        "    'parameters':{\n",
        "        'num_filters':{'values':[32,64]},\n",
        "        'activation':{'values':['ReLU','sigmoid','tanh','GELU','SiLU','Mish']},\n",
        "        'data_augmentation':{'values':['Yes','No']},\n",
        "        'batch_normalization':{'values':['Yes','No']},\n",
        "        'dropout':{'values':[0.2,0.3]},\n",
        "        'epochs':{'values':[5,10,15]},\n",
        "        'dense_neurons':{'values':[64,128,256,512]},\n",
        "        'learning_rate':{'values':[0.0001,0.00001]},\n",
        "        'freeze':{'values':['False','True']},\n",
        "        'freeze_value':{'values':[10,20,40]}\n",
        "    }\n",
        "}\n",
        "\n",
        "sweepId=wandb.sweep(sweep_config,project='cs23m035_DL_Assignment2')\n",
        "wandb.agent(sweepId,pretrain_model)"
      ],
      "metadata": {
        "id": "nodtjzd45eRS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}