{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1uGtUHkiGhSXJMKFxUH4IF9DzTgPwPlrT",
      "authorship_tag": "ABX9TyPWy8fn6i3eAsBl8FmmAUKt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parthasarathi1234/DL_assignment2/blob/main/DL_Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Wg3sGP-3ngwF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets, transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
        "!pip install wandb\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z-wh0YMGB5vh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128,128)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "train_dataset=datasets.ImageFolder(root='inaturalist_12K/train',transform=transform)\n",
        "test_dataset=datasets.ImageFolder(root='inaturalist_12K/val',transform=transform)\n",
        "\n",
        "train_indices, val_indices = train_test_split(list(range(len(train_dataset))), test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "val_sampler = SubsetRandomSampler(val_indices)\n",
        "\n",
        "# Creating DataLoader instances for training and validation\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, sampler=train_sampler)\n",
        "val_loader = DataLoader(train_dataset, batch_size=32, sampler=val_sampler)\n",
        "print(len(train_dataset))\n",
        "print(len(test_dataset))\n",
        "\n",
        "print(len(train_sampler))\n",
        "print(len(val_sampler))\n",
        "print(len(train_loader))\n",
        "print(len(val_loader))"
      ],
      "metadata": {
        "id": "JlHS_wmYRSil"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SmallCNN(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_filters,\n",
        "                 activation,\n",
        "                 data_augmentation,\n",
        "                 batch_normalization,\n",
        "                 dense_neurons,\n",
        "                 dropout\n",
        "                ):\n",
        "        self.activation=activation;\n",
        "        super(SmallCNN, self).__init__()\n",
        "\n",
        "        # 1st convolutional layer\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=num_filters, kernel_size=3, padding=1)\n",
        "        # 2nd convolutional layer\n",
        "        self.conv2 = nn.Conv2d(num_filters, num_filters * 2, kernel_size=3, padding=1)\n",
        "        # 3rd convolutional layer\n",
        "        self.conv3 = nn.Conv2d(num_filters * 2, num_filters * 4, kernel_size=3, padding=1)\n",
        "        # 4th convolutional layer\n",
        "        self.conv4 = nn.Conv2d(num_filters * 4, num_filters * 8, kernel_size=3, padding=1)\n",
        "        # 5th convolutional layer\n",
        "        self.conv5 = nn.Conv2d(num_filters * 8, num_filters * 16, kernel_size=3, padding=1)\n",
        "        # max pooling\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        # dense layer\n",
        "        self.fc1 = nn.Linear(num_filters * 16 * 4*4, dense_neurons)\n",
        "        # output dense layer\n",
        "        self.fc2 = nn.Linear(dense_neurons, 10)  # Output layer with 10 neurons for classification\n",
        "\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "        self.batch_norm=nn.BatchNorm2d(num_filters) if batch_normalization else None\n",
        "\n",
        "    def forward(self, x):\n",
        "        if(self.activation=='ReLU'):\n",
        "            x = self.pool(F.relu(self.conv1(x)))\n",
        "            x = self.pool(F.relu(self.conv2(x)))\n",
        "            x = self.pool(F.relu(self.conv3(x)))\n",
        "            x = self.pool(F.relu(self.conv4(x)))\n",
        "            x = self.pool(F.relu(self.conv5(x)))\n",
        "        if(self.activation=='sigmoid'):\n",
        "            x = self.pool(F.sigmoid(self.conv1(x)))\n",
        "            x = self.pool(F.sigmoid(self.conv2(x)))\n",
        "            x = self.pool(F.sigmoid(self.conv3(x)))\n",
        "            x = self.pool(F.sigmoid(self.conv4(x)))\n",
        "            x = self.pool(F.sigmoid(self.conv5(x)))\n",
        "        if(self.activation=='tanh'):\n",
        "            x = self.pool(F.tanh(self.conv1(x)))\n",
        "            x = self.pool(F.tanh(self.conv2(x)))\n",
        "            x = self.pool(F.tanh(self.conv3(x)))\n",
        "            x = self.pool(F.tanh(self.conv4(x)))\n",
        "            x = self.pool(F.tanh(self.conv5(x)))\n",
        "\n",
        "        x = x.view(-1, num_filters * 16 * 4*4)\n",
        "\n",
        "        if(self.activation=='ReLU'):\n",
        "            x = self.dropout(F.relu(self.fc1(x)))\n",
        "        if(self.activation=='sigmoid'):\n",
        "            x = self.dropout(F.sigmoid(self.fc1(x)))\n",
        "        if(self.activation=='tanh'):\n",
        "            x = self.dropout(F.tanh(self.fc1(x)))\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    def num_flat_features(self, x):\n",
        "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
        "        num_features = 1\n",
        "        for s in size:\n",
        "            num_features *= s\n",
        "        return num_features\n",
        "\n",
        "# Define hyperparameters\n",
        "\n",
        "\n",
        "\n",
        "#train network\n",
        "def train_network():\n",
        "    with wandb.init() as run:\n",
        "        config=wandb.config\n",
        "        cnn_model = SmallCNN(config.num_filters,\n",
        "                             config.activation,\n",
        "                             config.data_augmentation,\n",
        "                             config.batch_normalization,\n",
        "                             config.dense_neurons,\n",
        "                             config.dropout\n",
        "                            ).to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer=optim.Adam(cnn_model.parameters(),lr=config.learning_rate)\n",
        "\n",
        "    for i in range(config.epochs):\n",
        "        cnn_model.train()\n",
        "        train_loss=0.0\n",
        "        train_correct=0\n",
        "        train_total=0\n",
        "        for image,label in train_loader:\n",
        "            image=image.to(device=device)\n",
        "            label=label.to(device=device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            scores=cnn_model(image)\n",
        "            loss=criterion(scores,label)\n",
        "\n",
        "            loss.backward()\n",
        "            #gradient descent or adam step\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss+=loss.item()\n",
        "            _,predicted=scores.max(1)\n",
        "            train_total+=label.size(0)\n",
        "            train_correct+=predicted.eq(label).sum().item()\n",
        "        train_loss=train_loss/len(train_loader)\n",
        "        train_accuracy=100*train_correct/train_total\n",
        "\n",
        "        num_correct=0\n",
        "        num_loss=0\n",
        "        total=0\n",
        "        cnn_model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for x,y in val_loader:\n",
        "                x=x.to(device=device)\n",
        "                y=y.to(device=device)\n",
        "                # x=x.reshape(x.shape[0],-1)\n",
        "                scores=cnn_model(x)\n",
        "                loss=criterion(scores,y)\n",
        "\n",
        "                num_loss+=loss.item()\n",
        "                _,predictions=scores.max(1)\n",
        "                total+=y.size(0)\n",
        "                num_correct+=predictions.eq(y).sum().item()\n",
        "          # print(f'Got {num_correct}/{num_samples} with accuracy {float(num_correct)/float(num_samples)*100:.2f}')\n",
        "        # cnn_model.train()\n",
        "        val_accuracy=100*num_correct/total\n",
        "        val_loss/=len(val_loader)\n",
        "        wandb.log({\"Train_Accuracy\" : train_accuracy,\"Train_Loss\" : train_loss,\"Validation_acc\" : val_accuracy,\"validation_loss\" : val_loss,'epoch':i})\n",
        "        print(f\"Train_Accuracy : {train_accuracy},Train_Loss : {train_loss}, Validation_acc : {val_accuracy},validation_loss : {val_loss},epoch:{i}\")\n",
        "\n",
        "# Create the CNN model\n",
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ],
      "metadata": {
        "id": "NjZ1JCsECtZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config={\n",
        "    'method':'bayes',\n",
        "    'name':'Parthu',\n",
        "    'metric':{\n",
        "        'goal':'maximize',\n",
        "        'name':'Validation_acc',\n",
        "    },\n",
        "    'parameters':{\n",
        "        'num_filters':{'values':[32,64]},\n",
        "        'activation':{'values':['ReLU','sigmoid','tanh']},\n",
        "        'data_augmentation':{'values':['Yes','No']},\n",
        "        'batch_normalization':{'values':['Yes','No']},\n",
        "        'dropout':{'values':[0.2,0.3]},\n",
        "        'epochs':{'values':[5,10,15]},\n",
        "        'dense_neurons':{'values':[64,128,256,512]},\n",
        "        'learning_rate':{'values':[0.0001,0.00001]}\n",
        "    }\n",
        "}\n",
        "\n",
        "sweepId=wandb.sweep(sweep_config,project='cs23m035_DL_Assignment2')\n",
        "wandb.agent(sweepId,train_network)"
      ],
      "metadata": {
        "id": "wjDwKwSOEZcn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}